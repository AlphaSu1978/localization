{
  "noInstanceSelected": "No model instance selected",
  "resetToDefault": "Reset",
  "showAdvancedSettings": "Show advanced settings",
  "showAll": "All",
  "basicSettings": "Basic",
  "configSubtitle": "Load or save presets and experiment with model parameter overrides",
  "inferenceParameters/title": "Prediction Parameters",
  "inferenceParameters/info": "Experiment with parameters that impact the prediction.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Sampling",
  "basicTab": "Basic",
  "advancedTab": "Advanced",
  "advancedTab/title": "üß™ Advanced Configuration",
  "advancedTab/expandAll": "Expand all",
  "advancedTab/overridesTitle": "Config Overrides",
  "advancedTab/noConfigsText": "You have no unsaved changes - edit values above to see overrides here.",
  "loadInstanceFirst": "Load a model to view configurable parameters",
  "noListedConfigs": "No configurable parameters",
  "generationParameters/info": "Experiment with basic parameters which impact text generation.",
  "loadParameters/title": "Load Parameters",
  "loadParameters/description": "Settings to control the way the model is initialized and loaded into memory.",
  "loadParameters/reload": "Reload to apply changes",
  "discardChanges": "Discard changes",
  "loadModelToSeeOptions": "Load a model to see options",
  "schematicsError.title": "The config schematics contains errors in the following fields:",
  "llm.prediction.systemPrompt/title": "System Prompt",
  "llm.prediction.systemPrompt/description": "Use this field to provide background instructions to the model, such as a set of rules, constraints, or general requirements.",
  "llm.prediction.systemPrompt/subTitle": "Guidelines for the AI",
  "llm.prediction.temperature/title": "Temperature",
  "llm.prediction.temperature/subTitle": "How much randomness to introduce. 0 will yield the same result every time, while higher values will increase creativity and variance",
  "llm.prediction.temperature/info": "From llama.cpp help docs: \"The default value is <{{dynamicValue}}>, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.topKSampling/info": "From llama.cpp help docs:\n\nTop-k sampling is a text generation method that selects the next token only from the top k most likely tokens predicted by the model.\n\nIt helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output.\n\nA higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text.\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU Threads",
  "llm.prediction.llama.cpuThreads/subTitle": "Number of CPU threads to use during inference",
  "llm.prediction.llama.cpuThreads/info": "The number of threads to use during computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limit Response Length",
  "llm.prediction.maxPredictedTokens/subTitle": "Optionally cap the length of the AI's response",
  "llm.prediction.maxPredictedTokens/info": "Control the max length of the chatbot's response. Turn on to set a limit on the max length of a response, or turn off to let the chatbot decide when to stop.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximum response length (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "About {{maxWords}} words",
  "llm.prediction.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.repeatPenalty/info": "From llama.cpp help docs: \"Helps prevent the model from generating repetitive or monotonous text.\n\nA higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.\" ‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimum base probability for a token to be selected for output",
  "llm.prediction.minPSampling/info": "From llama.cpp help docs:\n\nThe minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P Sampling",
  "llm.prediction.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.topPSampling/info": "From llama.cpp help docs:\n\nTop-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p.\n\nThis method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from.\n\nA higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Must be in (0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stop Strings",
  "llm.prediction.stopStrings/subTitle": "Strings that should stop the model from generating more tokens",
  "llm.prediction.stopStrings/info": "Specific strings that when encountered will stop the model from generating more tokens",
  "llm.prediction.stopStrings/placeholder": "Enter a string and press ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Context Overflow",
  "llm.prediction.contextOverflowPolicy/subTitle": "How the model should behave when the conversation grows too large for it to handle",
  "llm.prediction.contextOverflowPolicy/info": "Decide what to do when the conversation exceeds the size of the model's working memory ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency Penalty",
  "llm.prediction.llama.presencePenalty/title": "Presence Penalty",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.llama.xtcProbability/title": "XTC Sampling Probability",
  "llm.prediction.llama.xtcProbability/subTitle": "The XTC (Exclude Top Choices) sampler will only be activated with this probability per generated token. XTC sampling can boost creativity and reduce clich√©s",
  "llm.prediction.llama.xtcProbability/info": "XTC (Exclude Top Choices) sampling will only be activated with this probability, per generated token. XTC sampling usually boosts creativity and reduces clich√©s",
  "llm.prediction.llama.xtcThreshold/title": "XTC Sampling Threshold",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.llama.xtcThreshold/info": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "From ONNX documentation:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Structured Output",
  "llm.prediction.structured/info": "Structured Output",
  "llm.prediction.structured/description": "Advanced: you can provide a [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) to enforce a particular output format from the model. Read the [documentation](https://lmstudio.ai/docs/advanced/structured-output) to learn more",
  "llm.prediction.tools/title": "Tool Use",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate/subTitle": "The format in which messages in chat are sent to the model. Changing this may introduce unexpected behavior - make sure you know what you're doing!",

  "llm.load.contextLength/title": "Context Length",
  "llm.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "llm.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "llm.load.contextLength/warning": "Setting a high value for context length can significantly impact memory usage",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random",
  "llm.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "Accelerates attention mechanisms for faster and more efficient processing",
  "llm.load.numExperts/title": "Number of Experts",
  "llm.load.numExperts/subTitle": "Number of experts to use in the model",
  "llm.load.numExperts/info": "The number of experts to use in the model",
  "llm.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Use FP16 For KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduces memory usage by storing cache in half-precision (FP16)",
  "llm.load.llama.tryMmap/title": "Try mmap()",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU Thread Pool Size",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K Cache Quantization Type",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",

  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Try mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",

  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "presetTooltip": {
    "included/title": "Preset Values",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "Load-time Configuration",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "to apply to",
    "separateLoad/description.3": ".",
    "excluded/title": "May not apply",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "Legacy Preset",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable.",
    "button/publish": "Publish",
    "button/pushUpdate": "Push Update",
    "button/export": "Export"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Before System",
      "manual.subfield.beforeSystem/placeholder": "Enter System prefix...",
      "manual.subfield.afterSystem/label": "After System",
      "manual.subfield.afterSystem/placeholder": "Enter System suffix...",
      "manual.subfield.beforeUser/label": "Before User",
      "manual.subfield.beforeUser/placeholder": "Enter User prefix...",
      "manual.subfield.afterUser/label": "After User",
      "manual.subfield.afterUser/placeholder": "Enter User suffix...",
      "manual.subfield.beforeAssistant/label": "Before Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Enter Assistant prefix...",
      "manual.subfield.afterAssistant/label": "After Assistant",
      "manual.subfield.afterAssistant/placeholder": "Enter Assistant suffix...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "Model supports up to",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop at Limit",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "Truncate Middle",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "llamaAccelerationSplitMode": {
      "singleGpu": "Single GPU",
      "layer": "Layer",
      "row": "Row"
    }
  },
  "saveConflictResolution": {
    "title": "Choose which values to include in the Preset",
    "description": "Pick and choose which values to keep",
    "instructions": "Click on a value to include it",
    "userValues": "Previous Value",
    "presetValues": "New Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "applyConflictResolution": {
    "title": "Which values to keep?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "Click on a value to keep it",
    "userValues": "Current Value",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "empty": "<Empty>",
  "presets": {
    "title": "Preset",
    "commitChanges": "Commit Changes",
    "commitChanges/description": "Commit your changes to the preset.",
    "commitChanges.manual": "New fields detected. You will be able to choose which changes to include in the preset.",
    "commitChanges.manual.hold.0": "Hold",
    "commitChanges.manual.hold.1": "to choose which changes to commit to the preset.",
    "commitChanges.saveAll.hold.0": "Hold",
    "commitChanges.saveAll.hold.1": "to save all changes.",
    "commitChanges.saveInPreset.hold.0": "Hold",
    "commitChanges.saveInPreset.hold.1": "to only save changes to fields that are already included in the preset.",
    "commitChanges/error": "Failed to commit changes to the preset.",
    "commitChanges.manual/description": "Choose which changes to include in the preset.",
    "saveAs": "Save As New...",
    "presetNamePlaceholder": "Enter a name for the preset...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotCommitChangesNoChanges": "No changes to commit.",
    "emptyNoUnsaved": "Select a Preset...",
    "emptyWithUnsaved": "Unsaved Preset",
    "saveEmptyWithUnsaved": "Save Preset As...",
    "saveConfirm": "Save",
    "saveCancel": "Cancel",
    "saving": "Saving...",
    "save/error": "Failed to save preset.",
    "deselect": "Deselect Preset",
    "deselect/error": "Failed to deselect preset.",
    "select/error": "Failed to select preset.",
    "delete/error": "Failed to delete preset.",
    "discardChanges": "Discard Unsaved",
    "discardChanges/info": "Discard all uncommitted changes and restore the preset to its original state",
    "newEmptyPreset": "+ New Preset",
    "importPreset": "Import",
    "contextMenuSelect": "Select Preset",
    "contextMenuDelete": "Delete",
    "contextMenuShare": "Push to Remote",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed ‚úÖ",
      "presetIsLive": "<custom-mono>{{name}}</custom-mono> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "Publishing your preset will make it public",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes",
      "descriptionLabel": "Description (optional)",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset JSON files or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (.json)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Download Preset {{owner}}/{{name}}",
      "button": "Download",
      "error": "Failed to download preset."
    }
  },

  "flashAttentionWarning": "Flash Attention is an experimental feature that may cause issues with some models. If you encounter problems, try disabling it.",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "advancedGpuSettings": "Advanced GPU Settings",
    "advancedGpuSettings.info": "If you're unsure, leave these at their default values",
    "advancedGpuSettings.reset": "Reset to default",
    "environmentVariables": {
      "title": "Env. Variables",
      "description": "Active environment variables during model lifetime.",
      "key.placeholder": "Select var...",
      "value.placeholder": "Value"
    },
    "mainGpu": {
      "title": "Main GPU",
      "description": "The GPU to prioritize for model computation.",
      "placeholder": "Select main GPU..."
    },
    "splitMode": {
      "title": "Split Mode",
      "description": "How to split model computation across GPUs.",
      "placeholder": "Select split mode..."
    }
  }
}
